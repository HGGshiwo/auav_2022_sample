{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv_bridge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb 单元格 2\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f726f732d67617a65626f2d3230227d/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f726f732d67617a65626f2d3230227d/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstd_msgs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmsg\u001b[39;00m \u001b[39mimport\u001b[39;00m Bool, Float32\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f726f732d67617a65626f2d3230227d/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcv_bridge\u001b[39;00m \u001b[39mimport\u001b[39;00m CvBridge\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f726f732d67617a65626f2d3230227d/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msensor_msgs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmsg\u001b[39;00m \u001b[39mimport\u001b[39;00m CameraInfo\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f726f732d67617a65626f2d3230227d/home/ubuntu/catkin_ws/src/auav_2022_sample/auav_2022_sample/scripts/train.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgeometry_msgs\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmsg\u001b[39;00m \u001b[39mimport\u001b[39;00m Point\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv_bridge'"
     ]
    }
   ],
   "source": [
    "import rospy\n",
    "import torch\n",
    "from A2C import A2C\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from std_msgs.msg import Bool, Float32\n",
    "from cv_bridge import CvBridge\n",
    "from sensor_msgs.msg import CameraInfo\n",
    "from geometry_msgs.msg import Point\n",
    "from mavros_msgs.msg import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self) -> None:\n",
    "        rospy.init_node(\"train\")\n",
    "\n",
    "        self.obs = None\n",
    "        self.rewards = None\n",
    "        self.terminated = None\n",
    "        self.ready = False\n",
    "        self.bridge = CvBridge()\n",
    "        self.actions = np.array(\n",
    "            [\n",
    "                [0, 0, 0],\n",
    "                [0, 0, 1],\n",
    "                [0, 0, -1],\n",
    "                [0, 1, 0],\n",
    "                [0, -1, 0],\n",
    "                [1, 0, 0],\n",
    "                [-1, 0, 0],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.finished_sub = rospy.Subscriber(\n",
    "            \"/rover/finished\", Bool, self.finished_callback\n",
    "        )\n",
    "        self.reward_sub = rospy.Subscriber(\"/inst_score\", Float32, self.reward_callback)\n",
    "        self.ready_sub = rospy.Subscriber(\"/drone/ready\", Bool, self.ready_callback)\n",
    "        self.fly_towards_pub = rospy.Publisher(\"/fly_towards\", Point, queue_size=10)\n",
    "        \n",
    "        print(\"wait for drone ready\")\n",
    "        while not self.ready:\n",
    "            rospy.sleep(10)\n",
    "\n",
    "    def reset(self):\n",
    "        # environment setup\n",
    "        self.obs = np.zeros((1, 3, 480, 640))\n",
    "        self.rewards = np.zeros((1, 1))\n",
    "        self.terminated = False\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"interact with the environment, get observation and reward\"\"\"\n",
    "        self.fly_towards(direction=self.actions[action[0]])\n",
    "\n",
    "        observation = self.observation  # 1*480*640*3\n",
    "        reward = np.array([self.reward])\n",
    "        terminated = [self.terminated]\n",
    "        return observation, reward, terminated, False, None\n",
    "\n",
    "    def finished_callback(self, msg):\n",
    "        self.terminated = msg.data\n",
    "\n",
    "    def reward_callback(self, msg):\n",
    "        self.reward = msg.data\n",
    "\n",
    "    def camera_info_callback(self, msg: CameraInfo):\n",
    "        \"\"\"Callback from camera projetion\"\"\"\n",
    "        self.camera_info = msg\n",
    "\n",
    "    def image_callback(self, msg):\n",
    "        if self.camera_info is None:\n",
    "            rospy.logerr(\"no camera info\")\n",
    "            return\n",
    "        obs = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\"bgr8\")\n",
    "        self.obs = np.array([obs])  # 480*640*3\n",
    "    \n",
    "    def ready_callback(self, msg):\n",
    "        self.ready = msg.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "envs = Env()\n",
    "\n",
    "# environment hyperparams\n",
    "n_envs = 1\n",
    "n_updates = 1000\n",
    "n_steps_per_update = 128\n",
    "randomize_domain = False\n",
    "\n",
    "# agent hyperparams\n",
    "gamma = 0.999\n",
    "lam = 0.95  # hyperparameter for GAE\n",
    "ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "# Note: the actor has a slower learning rate so that the value targets become\n",
    "# more stationary and are theirfore easier to estimate for the critic\n",
    "\n",
    "# environment setup\n",
    "obs = np.zeros((1, 3, 480, 640))\n",
    "rewards = np.zeros((1, 1))\n",
    "actions = np.array(\n",
    "    [[0, 0, 0], [0, 0, 1], [0, 0, -1], [0, 1, 0], [0, -1, 0], [1, 0, 0], [-1, 0, 0]]\n",
    ")\n",
    "\n",
    "# set the device\n",
    "use_cuda = False\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# init the agent\n",
    "obs_shape = envs.obs.shape\n",
    "action_shape = envs.actions.shape\n",
    "agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "return_queue = []\n",
    "\n",
    "for sample_phase in range(n_updates):\n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    # reset lists that collect experiences of an episode (sample phase)\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    # at the start of training reset all envs to get an initial state\n",
    "    if sample_phase == 0:\n",
    "        states, info = envs.reset()\n",
    "\n",
    "    # play n steps in our parallel environments to collect data\n",
    "    for step in range(n_steps_per_update):\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(\n",
    "            states\n",
    "        )\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        states, rewards, terminated, truncated, infos = envs.step(actions.cpu().numpy())\n",
    "\n",
    "        ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "\n",
    "        # add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "    # calculate the losses for actor and critic\n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_value_preds,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # update the actor and critic networks\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    # log the losses and entropy\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().mean().cpu().numpy())\n",
    "    return_queue.append(np.mean(ep_rewards))\n",
    "\n",
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the LunarLander-v2 environment \\n \\\n",
    "             (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
